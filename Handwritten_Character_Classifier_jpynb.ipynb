{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af301746"
      },
      "source": [
        "\n",
        "# Handwritten Character Classifier\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this project, we implement a Convolutional Neural Network (CNN) and a Random Forest Classifier to classify handwritten characters and digits. The goal is to create an efficient model that can accurately classify input images into one of the predefined categories, using state-of-the-art machine learning techniques.\n",
        "\n",
        "### Objective\n",
        "The main objective of this project is to design, implement, and compare both a CNN and Random Foreset Archetecture to understand each methods strengths and weaknesses.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xVh32suRvjG"
      },
      "source": [
        "#Configure the Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dhaaNpDARzSF"
      },
      "outputs": [],
      "source": [
        "# Required Imports\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nVFVoUTS2ps"
      },
      "source": [
        "#Preparing the Data\n",
        "\n",
        "### **Data Preparation and Processing**\n",
        "\n",
        "In this section, we will perform several crucial steps to prepare the data for training our Convolutional Neural Network (CNN). The **EMNIST** dataset will be used for handwritten character recognition. The following steps outline the process:\n",
        "\n",
        "1. **Downloading the EMNIST Dataset**: The dataset consists of handwritten letters and will be split into training and testing datasets. We will download the data and apply basic transformations to prepare it for training.\n",
        "\n",
        "2. **Visualizing Sample Data**: We'll display a few examples from the dataset to get an idea of what the data looks like before processing.\n",
        "\n",
        "3. **Image Orientation and Transformations**: A custom transformation will be applied to the images, which includes rotating and flipping the images to standardize their orientation. Additional normalization will also be applied to help with model training.\n",
        "\n",
        "4. **Calculating Dataset Statistics**: The mean and standard deviation of the training dataset will be calculated, which will be used for normalizing the images during training.\n",
        "\n",
        "5. **Processing and Sharding the Dataset**: To make it manageable and optimize training, the dataset will be divided into smaller chunks (shards). These shards will be saved as separate files, ready for use during model training.\n",
        "\n",
        "6. **Displaying Shard Samples**: After the dataset has been processed and saved in shards, we will display a sample image from one of the saved shards to ensure everything has been processed correctly.\n",
        "\n",
        "By the end of this section, the data will be preprocessed, normalized, and split into manageable shards, ready for use in training our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njLf0DWoUXGN"
      },
      "source": [
        "### Download the EMNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bSSQHSaNUBn5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 124800\n",
            "Testing set size: 20800\n"
          ]
        }
      ],
      "source": [
        "# ----- Configurations -----\n",
        "ROOT = Path('./data')\n",
        "OUTPUT_DIR = Path('./data/processed')\n",
        "BATCH_SIZE = 1024\n",
        "SHARD_SIZE = 10000\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# Download the EMNIST dataset (Letters)\n",
        "train_dataset = torchvision.datasets.EMNIST(\n",
        "    root='./data',\n",
        "    split='letters',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.EMNIST(\n",
        "    root='./data',\n",
        "    split='letters',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Check the dataset size\n",
        "print(f'Training set size: {len(train_dataset)}')\n",
        "print(f'Testing set size: {len(test_dataset)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGB3X3s8Uc25"
      },
      "source": [
        "### Visualize Some Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sdrxj-AkUMyo"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize some samples from the dataset\n",
        "def show_samples(dataset, num_samples=5):\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(num_samples):\n",
        "        image, label = dataset[i]\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(image.squeeze(), cmap='gray')\n",
        "        plt.title(f'Label: {label}')\n",
        "        plt.axis('off')\n",
        "\n",
        "show_samples(train_dataset, num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZmPlPuoUhdr"
      },
      "source": [
        "### Image Orientation and Transformation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5Wf7ZDWcUl9r"
      },
      "outputs": [],
      "source": [
        "def orient_images(img):\n",
        "    # Rotate the image by 90 degrees clockwise\n",
        "    oriented_image = TF.rotate(img, -90)\n",
        "    # Flip the image horizontally\n",
        "    oriented_image = TF.hflip(oriented_image)\n",
        "    return oriented_image\n",
        "\n",
        "def make_tf(mean, std):\n",
        "    ops = [\n",
        "        transforms.Lambda(orient_images),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        "    if mean is not None and std is not None:\n",
        "        ops += [transforms.Normalize([mean], [std])]\n",
        "    return transforms.Compose(ops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL2mtU21UqXo"
      },
      "source": [
        "### Compute Mean and Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1cwqRcNjUtEB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed Mean: 0.17222730815410614, Std: 0.3241918385028839\n"
          ]
        }
      ],
      "source": [
        "# Compute the mean and std of the dataset\n",
        "def compute_mean_std(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_samples += batch_samples\n",
        "\n",
        "    mean /= total_samples\n",
        "    std /= total_samples\n",
        "    return mean.item(), std.item()\n",
        "\n",
        "# Compute mean and std of the training set\n",
        "mean, std = compute_mean_std(train_dataset)\n",
        "print(f'Computed Mean: {mean}, Std: {std}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtKWS0rUUvOI"
      },
      "source": [
        "### Process and Save the Dataset in Shards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qRWjiCFgUzJI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved data\\processed\\train\\shard_000.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_001.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_002.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_003.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_004.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_005.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_006.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_007.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_008.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_009.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_010.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_011.pt with 10000 samples.\n",
            "Saved data\\processed\\train\\shard_012.pt with 4800 samples.\n",
            "Saved data\\processed\\test\\shard_000.pt with 10000 samples.\n",
            "Saved data\\processed\\test\\shard_001.pt with 10000 samples.\n",
            "Saved data\\processed\\test\\shard_002.pt with 800 samples.\n"
          ]
        }
      ],
      "source": [
        "# Remap labels from 1-26 to 0-25\n",
        "def remap_labels(label):\n",
        "    return label - 1\n",
        "\n",
        "# Process and save the dataset in shards\n",
        "def process_and_save_dataset(dataset, split_name, mean, std):\n",
        "    output_dir = OUTPUT_DIR / split_name\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    tf = make_tf(mean, std)\n",
        "    dataset.transform = tf\n",
        "\n",
        "    num_shards = (len(dataset) + SHARD_SIZE - 1) // SHARD_SIZE\n",
        "\n",
        "    for shard_idx in range(num_shards):\n",
        "        start_idx = shard_idx * SHARD_SIZE\n",
        "        end_idx = min((shard_idx + 1) * SHARD_SIZE, len(dataset))\n",
        "        shard_data = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):\n",
        "            image, label = dataset[i]\n",
        "            label = remap_labels(label)\n",
        "            shard_data.append((image.numpy(), label))\n",
        "\n",
        "        shard_path = output_dir / f'shard_{shard_idx:03d}.pt'\n",
        "        torch.save(shard_data, shard_path)\n",
        "        print(f'Saved {shard_path} with {len(shard_data)} samples.')\n",
        "\n",
        "# Process and save the training and test datasets\n",
        "process_and_save_dataset(train_dataset, 'train', mean, std)\n",
        "process_and_save_dataset(test_dataset, 'test', mean, std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NAlsxYgU00n"
      },
      "source": [
        "### Verify Shards Work by displaying a sample from the shard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nHyQ-Ao_U9Cx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Show a sample from a saved shard\n",
        "def show_image(image, label):\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def display_sample_from_shard(shard_path, sample_idx=0):\n",
        "    shard_data = torch.load(shard_path, weights_only=False, map_location='cpu')\n",
        "    image, label = shard_data[sample_idx]\n",
        "    show_image(image, label)\n",
        "\n",
        "# Example of displaying the first sample from the first shard of the training data\n",
        "display_sample_from_shard(OUTPUT_DIR / 'train' / 'shard_000.pt', sample_idx=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS8hLK6EVJsM"
      },
      "source": [
        "#CNN Model Definition and Training\n",
        "\n",
        "In this section, we define and implement the architecture for the Convolutional Neural Network (CNN) used in this project. The primary goal of the CNN is to classify handwritten characters into predefined categories. The following steps outline what is being done:\n",
        "\n",
        "## Defining the CNN Class:\n",
        "We define the CNN architecture by subclassing torch.nn.Module. This class contains the layers, activations, and forward pass operations that are required for the model to process input images and output class predictions.\n",
        "\n",
        "##Model Layers:\n",
        "The CNN architecture includes several convolutional layers followed by batch normalization and activation functions (ReLU). We also include max pooling layers to reduce the spatial dimensions of the feature maps.\n",
        "\n",
        "## Fully Connected Layers:\n",
        "After the convolutional layers, the feature maps are flattened and passed through fully connected (linear) layers. These layers are used to classify the output into one of the 26 classes corresponding to different characters.\n",
        "\n",
        "## Forward Pass:\n",
        "The forward method defines how the input is passed through the network. It includes the operations from the input layer through to the final output layer, which gives the prediction scores for each class.\n",
        "\n",
        "## Training the Model:\n",
        "The model is trained using the backpropagation algorithm. In this section, we initialize the model, set the loss function (cross-entropy loss), and define the optimizer. We then train the model on batches of data for several epochs, adjusting the weights based on the calculated gradients.\n",
        "\n",
        "## Evaluation:\n",
        "After training, the model is evaluated on the test dataset to check its performance and accuracy. We also save the best-performing model during training.\n",
        "\n",
        "By the end of this section, we will have a trained CNN model that can classify handwritten characters, which can be used for inference or further improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmb3ALzkWBqu"
      },
      "source": [
        "### Abstract CNN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o2tkyb6qV_Ra"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class CNNConfig:\n",
        "    input_channels: int\n",
        "    num_classes: int\n",
        "    conv_layers: Tuple[int, int, int]  # (out_channels, kernel_size, stride)\n",
        "    dropout: Optional[float] = None\n",
        "\n",
        "class AbstractCNN(nn.Module, ABC):\n",
        "    def __init__(self, config: CNNConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers: nn.Sequential\n",
        "        self.classifier: nn.Module\n",
        "        self.dropout: nn.Module = nn.Identity()\n",
        "        self.build()\n",
        "\n",
        "    @abstractmethod\n",
        "    def build(self) -> nn.Sequential:\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layers(x)\n",
        "        x = x.mean(dim=[2, 3])  # Global Average Pooling\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2xc7TULWIab"
      },
      "source": [
        "### Simple CNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2vCx7hnQWMP1"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(AbstractCNN):\n",
        "    def build(self) -> nn.Sequential:\n",
        "        c = self.config\n",
        "        w1, w2, w3 = c.conv_layers\n",
        "\n",
        "        # Block 1 28x28 -> 14x14\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(c.input_channels, w1, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(w1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(w1, w1, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(w1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Block 2 14x14 -> 7x7\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(w1, w2, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(w2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(w2, w2, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(w2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Feature Head 7x7 -> 7x7\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(w2, w3, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(w3),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Combine all layers\n",
        "        self.layers = nn.Sequential(\n",
        "            self.b1,\n",
        "            self.b2,\n",
        "            self.head\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        if c.dropout:\n",
        "            self.dropout = nn.Dropout(c.dropout)\n",
        "        self.classifier = nn.Linear(w3, c.num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXoCvdheWOpJ"
      },
      "source": [
        "### Sample Model and Testing Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3Rz-Xh4rWanG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits shape: torch.Size([8, 26])\n",
            "param count: 142522\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "cfg = CNNConfig(input_channels=1, num_classes=26, conv_layers=(32, 64, 128), dropout=0.3)\n",
        "model = SimpleCNN(cfg)\n",
        "x = torch.randn(8, 1, 28, 28)        # batch of 8, preprocessed images\n",
        "y = model(x)\n",
        "print(\"logits shape:\", y.shape)      # expect [8, 26]\n",
        "print(\"param count:\", sum(p.numel() for p in model.parameters()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht-uNw5BXZwi"
      },
      "source": [
        "### Data Loader from Shards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ItfG444fXjeN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from typing import List, Sequence\n",
        "\n",
        "class PTShardDataset(Dataset):\n",
        "    def __init__(self, shard_paths: Sequence[Path]):\n",
        "        self.items: List[tuple] = []\n",
        "        for p in shard_paths:\n",
        "            chunk = torch.load(p, map_location='cpu', weights_only=False)\n",
        "            if not isinstance(chunk, list):\n",
        "                raise ValueError(f\"Expected list in shard {p}, got {type(chunk)}\")\n",
        "            self.items.extend(chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_np, y = self.items[idx]\n",
        "        x = torch.as_tensor(x_np, dtype=torch.float32)\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(0)  # Add channel dimension for grayscale images\n",
        "        elif x.ndim == 3 and x.shape[0] != 1:\n",
        "            x = x.permute(2, 0, 1).contiguous()  # Change HWC to CHW\n",
        "\n",
        "        y = int(y)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    @staticmethod\n",
        "    def create_dataloader(proc_dir: Path, batch_train=128, batch_eval=256, val_frac=0.1, seed=42):\n",
        "        train_dir = proc_dir / 'train'\n",
        "        test_dir = proc_dir / 'test'\n",
        "\n",
        "        train_shards = sorted(train_dir.glob('shard_*.pt'))\n",
        "        test_shards = sorted(test_dir.glob('shard_*.pt'))\n",
        "\n",
        "        full_train = PTShardDataset(train_shards)\n",
        "        test = PTShardDataset(test_shards)\n",
        "\n",
        "        if not train_shards or not test_shards:\n",
        "            raise FileNotFoundError(f\"No shards found in {train_dir} or {test_dir}\")\n",
        "\n",
        "        # Split training shards into training and validation sets\n",
        "        num_val_shards = int(len(full_train) * val_frac)\n",
        "        num_train_shards = len(full_train) - num_val_shards\n",
        "\n",
        "        g = torch.Generator().manual_seed(seed)\n",
        "        train_ds, val_ds = torch.utils.data.random_split(\n",
        "            full_train, [num_train_shards, num_val_shards], generator=g\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_train, shuffle=True, num_workers=4, pin_memory=False)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_eval, shuffle=False, num_workers=4, pin_memory=False)\n",
        "        test_loader = DataLoader(test, batch_size=batch_eval, shuffle=False, num_workers=4, pin_memory=False)\n",
        "\n",
        "        return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHW6T9XyXyOf"
      },
      "source": [
        "### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iOicrRSdX0lR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def run_epoch(model, loader, criterion, optimizer=None, device='cpu'):\n",
        "    train = optimizer is not None\n",
        "    model.train(train)\n",
        "    total, correct, loss = 0, 0, 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = torch.as_tensor(y, device=device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return loss / total, correct / total\n",
        "\n",
        "def evaluate_confusion(model, loader, num_classes, device='cpu'):\n",
        "    model.eval()\n",
        "    confusion = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = torch.as_tensor(y, device=device)\n",
        "\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            for t, p in zip(y.view(-1), pred.view(-1)):\n",
        "                confusion[int(t), int(p)] += 1\n",
        "\n",
        "    per_class_acc = confusion.diagonal() / np.clip(confusion.sum(axis=1), 1, a_max=None)\n",
        "\n",
        "    return confusion, per_class_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0d_ncg3YAZx"
      },
      "source": [
        "### Model Training and Saving Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSet86ccYDug"
      },
      "outputs": [],
      "source": [
        "# Training loop and evaluation for the model\n",
        "train_loader, val_loader, test_loader = PTShardDataset.create_dataloader(Path('./data/processed'), batch_train=128, batch_eval=256)\n",
        "\n",
        "model = SimpleCNN(CNNConfig(input_channels=1, num_classes=26, conv_layers=(32,64,128), dropout=0.3)).to('cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
        "\n",
        "epochs = 15\n",
        "best_val = 0.0\n",
        "save_path = Path('best_model.pt')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device='cpu')\n",
        "    val_loss, val_acc = run_epoch(model, val_loader, criterion, None, device='cpu')\n",
        "\n",
        "    prev_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    scheduler.step(val_acc)  # uses validation accuracy\n",
        "    curr_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    if curr_lr != prev_lr:\n",
        "        print(f\"LR reduced: {prev_lr:.2e} -> {curr_lr:.2e}\")\n",
        "\n",
        "    print(f'Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f'New best model saved with Val Acc={best_val:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEyUt3fxYG24"
      },
      "source": [
        "### Model Evaluation and Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVbmXP6iYJdU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import json\n",
        "\n",
        "def test_model(model, test_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "\n",
        "    correct_by_class = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = torch.as_tensor(y, device=device)\n",
        "\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += x.size(0)\n",
        "\n",
        "            for t, p in zip(y.view(-1), pred.view(-1)):\n",
        "                t = int(t)\n",
        "                if t not in correct_by_class:\n",
        "                    correct_by_class[t] = {'correct': 0, 'total': 0}\n",
        "                if t == int(p):\n",
        "                    correct_by_class[t]['correct'] += 1\n",
        "                correct_by_class[t]['total'] += 1\n",
        "\n",
        "    overall_acc = correct / total\n",
        "    print(f'Test Accuracy: {overall_acc * 100:.2f}%')\n",
        "    for cls, stats in correct_by_class.items():\n",
        "        class_acc = stats['correct'] / stats['total']\n",
        "        print(f'Class {cls}: Accuracy: {class_acc * 100:.2f}%')\n",
        "\n",
        "    return overall_acc, correct_by_class\n",
        "\n",
        "# Example usage for evaluation\n",
        "model.load_state_dict(torch.load('best_model.pt', map_location='cpu'))\n",
        "_, _, test_loader = PTShardDataset.create_dataloader(Path('./data/processed'), batch_train=128, batch_eval=256)\n",
        "overall_acc, correct_by_class = test_model(model, test_loader, device='cpu')\n",
        "\n",
        "# Plot the test accuracy by class\n",
        "num_classes = 26\n",
        "letters = list(string.ascii_uppercase)\n",
        "\n",
        "acc = []\n",
        "for i in range(num_classes):\n",
        "    if i in correct_by_class:\n",
        "        stats = correct_by_class[i]\n",
        "        class_acc = stats['correct'] / stats['total']\n",
        "    else:\n",
        "        class_acc = 0.0\n",
        "    acc.append(class_acc * 100)\n",
        "\n",
        "plt.bar(range(num_classes), acc)\n",
        "plt.xticks(range(num_classes), letters)\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy by Class')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest Model Training and Evaluation\n",
        "\n",
        "In this section, we define and train a Random Forest model to classify handwritten characters from the EMNIST dataset. We also evaluate its performance on the test set, compute the accuracy, and display the results using visualizations.\n",
        "\n",
        "## RFModel Class:\n",
        "\n",
        "A wrapper around the RandomForestClassifier from sklearn. This class handles training, prediction, saving, loading, and extracting feature importances from the trained model.\n",
        "\n",
        "It includes methods for fitting the model to training data, making predictions, saving the trained model to disk, and loading it back.\n",
        "\n",
        "## Training Pipeline:\n",
        "\n",
        "The data is loaded from shards using the PTShardDataset class, which is similar to the one used for the CNN model.\n",
        "\n",
        "The training data is converted to numpy arrays and passed to the RandomForestClassifier for training.\n",
        "\n",
        "After training, the model is saved and evaluated on both the validation and test datasets.\n",
        "\n",
        "## Evaluation:\n",
        "\n",
        "The model's performance is evaluated using accuracy and per-class accuracy.\n",
        "\n",
        "We also visualize the per-class accuracy for the test set using a bar chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RF Model Class and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
